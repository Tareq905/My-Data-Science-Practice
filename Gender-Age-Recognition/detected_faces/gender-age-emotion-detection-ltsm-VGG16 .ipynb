{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Input, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_model = load_model('gender_age_emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: vgg16. Existing layers are: ['conv2d_21', 'activation_30', 'batch_normalization_27', 'conv2d_22', 'activation_31', 'batch_normalization_28', 'max_pooling2d_11', 'dropout_17', 'conv2d_23', 'activation_32', 'batch_normalization_29', 'conv2d_24', 'activation_33', 'batch_normalization_30', 'max_pooling2d_12', 'dropout_18', 'conv2d_25', 'activation_34', 'batch_normalization_31', 'conv2d_26', 'activation_35', 'batch_normalization_32', 'max_pooling2d_13', 'dropout_19', 'conv2d_27', 'activation_36', 'batch_normalization_33', 'conv2d_28', 'activation_37', 'batch_normalization_34', 'max_pooling2d_14', 'dropout_20', 'flatten_4', 'dense_10', 'activation_38', 'batch_normalization_35', 'dropout_21', 'dense_11', 'activation_39', 'batch_normalization_36', 'dropout_22', 'dense_12', 'activation_40'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-544ea87d2135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extract VGG16 base from the pre-trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvgg16_base\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vgg16'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvgg16_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mget_layer\u001b[1;34m(self, name, index)\u001b[0m\n\u001b[0;32m   3462\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   3465\u001b[0m                 \u001b[1;34mf\"No such layer: {name}. Existing layers are: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m                 \u001b[1;34mf\"{list(layer.name for layer in self.layers)}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No such layer: vgg16. Existing layers are: ['conv2d_21', 'activation_30', 'batch_normalization_27', 'conv2d_22', 'activation_31', 'batch_normalization_28', 'max_pooling2d_11', 'dropout_17', 'conv2d_23', 'activation_32', 'batch_normalization_29', 'conv2d_24', 'activation_33', 'batch_normalization_30', 'max_pooling2d_12', 'dropout_18', 'conv2d_25', 'activation_34', 'batch_normalization_31', 'conv2d_26', 'activation_35', 'batch_normalization_32', 'max_pooling2d_13', 'dropout_19', 'conv2d_27', 'activation_36', 'batch_normalization_33', 'conv2d_28', 'activation_37', 'batch_normalization_34', 'max_pooling2d_14', 'dropout_20', 'flatten_4', 'dense_10', 'activation_38', 'batch_normalization_35', 'dropout_21', 'dense_11', 'activation_39', 'batch_normalization_36', 'dropout_22', 'dense_12', 'activation_40']."
     ]
    }
   ],
   "source": [
    "# Extract VGG16 base from the pre-trained model\n",
    "vgg16_base = pretrained_model.get_layer('vgg16')\n",
    "vgg16_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers to integrate with LSTM\n",
    "flatten = Flatten()(vgg16_base.output)\n",
    "fc1 = Dense(256, activation=\"relu\")(flatten)  # Reduce to match LSTM input\n",
    "\n",
    "time_distributed = TimeDistributed(Dense(256, activation=\"relu\"))  # TimeDistributed layer for LSTM compatibility\n",
    "lstm_input = Input(shape=(10, 256))\n",
    "lstm_layer = LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)(lstm_input)\n",
    "output_layer = Dense(5, activation=\"softmax\")(lstm_layer)  # Adjusted for multiple emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete model\n",
    "smart_model = Model(inputs=[vgg16_base.input, lstm_input], outputs=output_layer)\n",
    "smart_model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess image\n",
    "def preprocess_image(frame):\n",
    "    resized_frame = cv2.resize(frame, (224, 224))\n",
    "    normalized_frame = resized_frame / 255.0  # Normalize to [0,1]\n",
    "    return np.expand_dims(normalized_frame, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map predictions to labels\n",
    "def map_predictions(predictions):\n",
    "    gender = \"Male\" if predictions[0] > 0.5 else \"Female\"\n",
    "    age = int(predictions[1] * 100)  # Age approximation based on prediction\n",
    "    emotions = [\"Happy\", \"Sad\", \"Angry\", \"Neutral\", \"Surprised\"]\n",
    "    emotion = emotions[np.argmax(predictions[2:])]\n",
    "    return gender, age, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth predictions\n",
    "def smooth_predictions(history, new_prediction, alpha=0.8):\n",
    "    return alpha * np.array(history) + (1 - alpha) * np.array(new_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 300ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'KerasTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1ee69f94414f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Start the webcam prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mcapture_and_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1ee69f94414f>\u001b[0m in \u001b[0;36mcapture_and_predict\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprocessed_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mvgg16_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg16_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mreduced_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvgg16_features\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Reduce features to match LSTM input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Maintain sequence length for LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'KerasTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Function to capture webcam feed and make predictions\n",
    "def capture_and_predict():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # To store sequences for LSTM\n",
    "    sequence = []\n",
    "    max_sequence_length = 10\n",
    "    prediction_history = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess frame\n",
    "        processed_frame = preprocess_image(frame)\n",
    "        vgg16_features = vgg16_base.predict(processed_frame)\n",
    "        reduced_features = fc1(vgg16_features)  # Reduce features to match LSTM input\n",
    "\n",
    "        # Maintain sequence length for LSTM\n",
    "        sequence.append(reduced_features.flatten())\n",
    "        if len(sequence) > max_sequence_length:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == max_sequence_length:\n",
    "            # Convert sequence to proper LSTM input shape\n",
    "            lstm_input_data = np.expand_dims(sequence, axis=0)\n",
    "\n",
    "            # Predict using the smart model\n",
    "            raw_predictions = smart_model.predict([processed_frame, lstm_input_data])[0]\n",
    "\n",
    "            # Smooth predictions for stability\n",
    "            if prediction_history is None:\n",
    "                prediction_history = raw_predictions\n",
    "            else:\n",
    "                prediction_history = smooth_predictions(prediction_history, raw_predictions)\n",
    "\n",
    "            # Map predictions to labels\n",
    "            gender, age, emotion = map_predictions(prediction_history)\n",
    "\n",
    "            # Overlay predictions on the frame\n",
    "            cv2.putText(frame, f\"Gender: {gender}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Age: {age}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Emotion: {emotion}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display webcam feed\n",
    "        cv2.imshow(\"Webcam Feed\", frame)\n",
    "\n",
    "        # Break loop on 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start the webcam prediction\n",
    "capture_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
