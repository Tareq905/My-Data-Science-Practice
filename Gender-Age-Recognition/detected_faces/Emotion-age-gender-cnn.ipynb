{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"gender_age_emotion_model.h5\")\n",
    "input_shape = (48, 48) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mediapipe face detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_face(face):\n",
    "    \"\"\"Preprocess the face for model input.\"\"\"\n",
    "    if face is None or face.size == 0:\n",
    "        return None  # Invalid face region\n",
    "    face_resized = cv2.resize(face, input_shape)  # Resize to model input shape\n",
    "    face_normalized = face_resized / 255.0  # Normalize pixel values\n",
    "    face_expanded = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n",
    "    return face_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: in user code:\n",
      "\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\TAREQ\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n",
      "    \n",
      "    Input 0 of layer \"conv2d_21\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 48, 48, 3)\n",
      "    \n",
      "    Call arguments received by layer 'sequential_4' (type Sequential):\n",
      "      • inputs=tf.Tensor(shape=(None, 48, 48, 3), dtype=float32)\n",
      "      • training=False\n",
      "      • mask=None\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "def live_gender_age_detection():\n",
    "    \"\"\"Real-time gender and age detection.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # Start webcam\n",
    "    try:\n",
    "        with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Failed to capture frame from webcam. Exiting...\")\n",
    "                    break\n",
    "\n",
    "                # Convert frame to RGB for Mediapipe\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_detection.process(rgb_frame)\n",
    "\n",
    "                if results.detections:\n",
    "                    for detection in results.detections:\n",
    "                        # Get bounding box from Mediapipe detection\n",
    "                        bboxC = detection.location_data.relative_bounding_box\n",
    "                        h, w, _ = frame.shape\n",
    "                        x1, y1 = int(bboxC.xmin * w), int(bboxC.ymin * h)\n",
    "                        x2, y2 = int((bboxC.xmin + bboxC.width) * w), int((bboxC.ymin + bboxC.height) * h)\n",
    "\n",
    "                        # Extract face from frame\n",
    "                        face = frame[y1:y2, x1:x2]\n",
    "                        face_processed = preprocess_face(face)\n",
    "\n",
    "                        if face_processed is None:\n",
    "                            continue  # Skip invalid face regions\n",
    "\n",
    "                        # Make predictions\n",
    "                        predictions = model.predict(face_processed)[0]\n",
    "                        gender_pred = predictions[:2]  # Assuming first two outputs are gender\n",
    "                        age_pred = predictions[2:]  # Assuming rest are age bins\n",
    "\n",
    "                        # Interpret predictions\n",
    "                        gender = \"Male\" if gender_pred[0] > gender_pred[1] else \"Female\"\n",
    "                        predicted_age = int(np.dot(np.arange(101), age_pred))  # Weighted average for age\n",
    "\n",
    "                        # Draw bounding box and predictions on frame\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(frame, f\"Gender: {gender}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "                        cv2.putText(frame, f\"Age: {predicted_age}\", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "\n",
    "                # Display the frame\n",
    "                cv2.imshow(\"Gender and Age Detection\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    print(\"Exiting...\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        sys.exit(0)  # Ensure proper termination\n",
    "\n",
    "# Run the detection\n",
    "live_gender_age_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
