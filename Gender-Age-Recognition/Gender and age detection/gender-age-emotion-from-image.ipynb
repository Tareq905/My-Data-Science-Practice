{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, ConvLSTM2D, BatchNormalization\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained models\n",
    "age_net = cv2.dnn.readNetFromCaffe('age_deploy.prototxt', 'age_net.caffemodel')\n",
    "gender_net = cv2.dnn.readNetFromCaffe('gender_deploy.prototxt', 'gender_net.caffemodel')\n",
    "face_net = cv2.dnn.readNetFromTensorflow('opencv_face_detector_uint8.pb', 'opencv_face_detector.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model labels\n",
    "age_labels = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "gender_labels = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image):\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect face\n",
    "def detect_face(image):\n",
    "    face_net.setInput(preprocess_image(image))\n",
    "    detections = face_net.forward()\n",
    "    h, w = image.shape[:2]\n",
    "    faces = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (x, y, x1, y1) = box.astype('int')\n",
    "            face = image[y:y1, x:x1]\n",
    "            faces.append((x, y, x1, y1, face))\n",
    "\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict age and gender\n",
    "def predict_age_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), (78.4263377603, 87.7689143744, 114.895847746))\n",
    "\n",
    "    gender_net.setInput(blob)\n",
    "    gender = gender_labels[np.argmax(gender_net.forward())]\n",
    "\n",
    "    age_net.setInput(blob)\n",
    "    age = age_labels[np.argmax(age_net.forward())]\n",
    "\n",
    "    return gender, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Emotion detection model\n",
    "emotion_model = load_model('emotion_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict emotion\n",
    "def predict_emotion(face):\n",
    "    gray_face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    resized_face = cv2.resize(gray_face, (64, 64)) / 255.0  \n",
    "    reshaped_face = resized_face.reshape(1, 64, 64, 1)  \n",
    "    emotion_prediction = emotion_model.predict(reshaped_face)\n",
    "    emotion = np.argmax(emotion_prediction)\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and run detection\n",
    "image = cv2.imread('images.jpg')\n",
    "faces = detect_face(image)\n",
    "\n",
    "for (x, y, x1, y1, face) in faces:\n",
    "    gender, age = predict_age_gender(face)\n",
    "    #emotion = predict_emotion(face)\n",
    "\n",
    "    label = f\"{gender}, {age}\"\n",
    "    cv2.rectangle(image, (x, y), (x1, y1), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "cv2.imshow('Gender, Age', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
